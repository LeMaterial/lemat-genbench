#!/usr/bin/env python3
"""Utility functions for working with LeMat-Bulk fingerprints.

This module provides utilities for loading, querying, and analyzing
fingerprint data generated by the preprocessing pipeline.
"""

import pickle
import time
from collections import Counter
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Union

import numpy as np
import pandas as pd
from pymatgen.core.structure import Structure
from pymatgen.io.cif import CifParser

from lemat_genbench.fingerprinting import get_augmented_fingerprint
from lemat_genbench.utils.logging import logger


class FingerprintDatabase:
    """Efficient database for fingerprint operations."""
    
    def __init__(self, data_dir: Union[str, Path]):
        """Initialize the fingerprint database.
        
        Parameters
        ----------
        data_dir : str or Path
            Directory containing fingerprint files.
        """
        self.data_dir = Path(data_dir)
        self.unique_fingerprints: Set[str] = set()
        self.fingerprint_mapping: Dict[str, str] = {}
        self.structure_index: Optional[pd.DataFrame] = None
        self._loaded = False
    
    def load_unique_fingerprints(self, force_reload: bool = False) -> Set[str]:
        """Load unique fingerprints from file.
        
        Parameters
        ----------
        force_reload : bool, default=False
            Force reload even if already loaded.
            
        Returns
        -------
        Set[str]
            Set of unique fingerprint strings.
        """
        if self._loaded and not force_reload:
            return self.unique_fingerprints
        
        # Try parquet file first (new format)
        unique_fp_parquet = self.data_dir / "unique_fingerprints.parquet"
        unique_fp_pkl = self.data_dir / "unique_fingerprints.pkl"
        
        if unique_fp_parquet.exists():
            logger.info(f"Loading unique fingerprints from {unique_fp_parquet}")
            df = pd.read_parquet(unique_fp_parquet)
            # Extract fingerprints from 'values' column
            if 'values' in df.columns:
                self.unique_fingerprints = set(df['values'].tolist())
            else:
                # Fallback: use first column if 'values' column not found
                self.unique_fingerprints = set(df.iloc[:, 0].tolist())
            logger.info(f"Loaded {len(self.unique_fingerprints):,} unique fingerprints from parquet")
            
        elif unique_fp_pkl.exists():
            logger.info(f"Loading unique fingerprints from {unique_fp_pkl}")
            with open(unique_fp_pkl, 'rb') as f:
                self.unique_fingerprints = pickle.load(f)
            logger.info(f"Loaded {len(self.unique_fingerprints):,} unique fingerprints from pickle")
            
        else:
            logger.warning(f"Unique fingerprints file not found: {unique_fp_parquet} or {unique_fp_pkl}")
            logger.info("Run combine_lemat_fingerprints.py first to create this file")
            self.unique_fingerprints = set()
        
        self._loaded = True
        return self.unique_fingerprints
    
    def load_fingerprint_mapping(self) -> Dict[str, str]:
        """Load immutable_id to fingerprint mapping.
        
        Returns
        -------
        Dict[str, str]
            Mapping from immutable_id to fingerprint_string.
        """
        mapping_file = self.data_dir / "fingerprint_mapping.pkl"
        
        if mapping_file.exists():
            logger.info(f"Loading fingerprint mapping from {mapping_file}")
            with open(mapping_file, 'rb') as f:
                self.fingerprint_mapping = pickle.load(f)
            logger.info(f"Loaded {len(self.fingerprint_mapping):,} fingerprint mappings")
        else:
            logger.warning(f"Fingerprint mapping file not found: {mapping_file}")
            self.fingerprint_mapping = {}
        
        return self.fingerprint_mapping
    
    def load_structure_index(self) -> pd.DataFrame:
        """Load structure metadata index.
        
        Returns
        -------
        pd.DataFrame
            Structure metadata index.
        """
        index_file = self.data_dir / "structure_index.parquet"
        
        if index_file.exists():
            logger.info(f"Loading structure index from {index_file}")
            self.structure_index = pd.read_parquet(index_file)
            logger.info(f"Loaded index for {len(self.structure_index):,} structures")
        else:
            logger.warning(f"Structure index file not found: {index_file}")
            # Try CSV fallback
            csv_file = self.data_dir / "structure_index.csv"
            if csv_file.exists():
                logger.info(f"Loading structure index from CSV: {csv_file}")
                self.structure_index = pd.read_csv(csv_file)
            else:
                logger.warning("No structure index found")
                self.structure_index = pd.DataFrame()
        
        return self.structure_index
    
    def is_fingerprint_novel(self, fingerprint: str) -> bool:
        """Check if a fingerprint is novel.
        
        Parameters
        ----------
        fingerprint : str
            Fingerprint string to check.
            
        Returns
        -------
        bool
            True if fingerprint is novel (not in reference set).
        """
        if not self._loaded:
            self.load_unique_fingerprints()
        
        return fingerprint not in self.unique_fingerprints
    
    def get_fingerprint_by_id(self, immutable_id: str) -> Optional[str]:
        """Get fingerprint for a given immutable_id.
        
        Parameters
        ----------
        immutable_id : str
            Immutable ID to look up.
            
        Returns
        -------
        str or None
            Fingerprint string if found, None otherwise.
        """
        if not self.fingerprint_mapping:
            self.load_fingerprint_mapping()
        
        return self.fingerprint_mapping.get(immutable_id)
    
    def search_by_elements(self, elements: List[str]) -> pd.DataFrame:
        """Search structures by element composition.
        
        Parameters
        ----------
        elements : List[str]
            List of element symbols to search for.
            
        Returns
        -------
        pd.DataFrame
            Matching structures from the index.
        """
        if self.structure_index is None:
            self.load_structure_index()
        
        if self.structure_index.empty:
            return pd.DataFrame()
        
        # Convert elements to set for comparison
        target_elements = set(elements)
        
        # Filter structures that contain exactly these elements
        mask = self.structure_index['elements'].apply(
            lambda x: set(x) == target_elements if isinstance(x, list) else False
        )
        
        return self.structure_index[mask].copy()
    
    def search_by_formula(self, formula: str) -> pd.DataFrame:
        """Search structures by chemical formula.
        
        Parameters
        ----------
        formula : str
            Chemical formula to search for.
            
        Returns
        -------
        pd.DataFrame
            Matching structures from the index.
        """
        if self.structure_index is None:
            self.load_structure_index()
        
        if self.structure_index.empty:
            return pd.DataFrame()
        
        mask = self.structure_index['formula'] == formula
        return self.structure_index[mask].copy()
    
    def search_by_spacegroup(self, spacegroup: int) -> pd.DataFrame:
        """Search structures by space group number.
        
        Parameters
        ----------
        spacegroup : int
            Space group number to search for.
            
        Returns
        -------
        pd.DataFrame
            Matching structures from the index.
        """
        if self.structure_index is None:
            self.load_structure_index()
        
        if self.structure_index.empty:
            return pd.DataFrame()
        
        mask = self.structure_index['spacegroup_number'] == spacegroup
        return self.structure_index[mask].copy()
    
    def get_statistics(self) -> Dict:
        """Get database statistics.
        
        Returns
        -------
        Dict
            Database statistics.
        """
        stats = {
            "unique_fingerprints_count": len(self.unique_fingerprints),
            "fingerprint_mappings_count": len(self.fingerprint_mapping),
            "structure_index_count": len(self.structure_index) if self.structure_index is not None else 0
        }
        
        if self.structure_index is not None and not self.structure_index.empty:
            stats.update({
                "unique_formulas": self.structure_index['formula'].nunique(),
                "unique_spacegroups": self.structure_index['spacegroup_number'].nunique(),
                "element_counts": {
                    "min": self.structure_index['num_elements'].min(),
                    "max": self.structure_index['num_elements'].max(),
                    "mean": self.structure_index['num_elements'].mean()
                }
            })
        
        return stats


def check_fingerprint_files(data_dir: Union[str, Path]) -> Dict[str, bool]:
    """Check which fingerprint files exist in a directory.
    
    Parameters
    ----------
    data_dir : str or Path
        Directory to check.
        
    Returns
    -------
    Dict[str, bool]
        Dictionary indicating which files exist.
    """
    data_dir = Path(data_dir)
    
    files_to_check = {
        "unique_fingerprints": "unique_fingerprints.pkl",
        "fingerprint_mapping": "fingerprint_mapping.pkl", 
        "structure_index_parquet": "structure_index.parquet",
        "structure_index_csv": "structure_index.csv",
        "combined_parquet": "lemat_bulk_fingerprints_combined.parquet",
        "combined_csv": "lemat_bulk_fingerprints_combined.csv",
        "processing_summary": "processing_summary.json",
        "combination_summary": "combination_summary.json"
    }
    
    file_status = {}
    for key, filename in files_to_check.items():
        file_path = data_dir / filename
        file_status[key] = file_path.exists()
        if file_status[key]:
            # Get file size for info
            size_mb = file_path.stat().st_size / (1024 * 1024)
            file_status[f"{key}_size_mb"] = round(size_mb, 2)
    
    return file_status


def validate_fingerprints_integrity(data_dir: Union[str, Path]) -> Dict[str, any]:
    """Validate integrity of fingerprint files.
    
    Parameters
    ----------
    data_dir : str or Path
        Directory containing fingerprint files.
        
    Returns
    -------
    Dict[str, any]
        Validation results and statistics.
    """
    data_dir = Path(data_dir)
    validation_results = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "statistics": {}
    }
    
    try:
        # Load database
        db = FingerprintDatabase(data_dir)
        
        # Check unique fingerprints
        unique_fps = db.load_unique_fingerprints()
        validation_results["statistics"]["unique_fingerprints"] = len(unique_fps)
        
        if len(unique_fps) == 0:
            validation_results["errors"].append("No unique fingerprints found")
            validation_results["valid"] = False
        
        # Check fingerprint mapping
        fp_mapping = db.load_fingerprint_mapping()
        validation_results["statistics"]["fingerprint_mappings"] = len(fp_mapping)
        
        # Cross-validate unique fingerprints vs mapping
        if unique_fps and fp_mapping:
            mapping_fps = set(fp_mapping.values())
            if not unique_fps.issubset(mapping_fps) and not mapping_fps.issubset(unique_fps):
                validation_results["warnings"].append(
                    "Mismatch between unique fingerprints and mapping fingerprints"
                )
        
        # Check structure index
        structure_index = db.load_structure_index()
        if not structure_index.empty:
            validation_results["statistics"]["structure_index_entries"] = len(structure_index)
            
            # Check for required columns
            required_cols = ['immutable_id', 'fingerprint_string', 'formula']
            missing_cols = [col for col in required_cols if col not in structure_index.columns]
            if missing_cols:
                validation_results["errors"].append(f"Missing columns in structure index: {missing_cols}")
                validation_results["valid"] = False
            
            # Check for duplicates
            if structure_index['immutable_id'].duplicated().any():
                validation_results["warnings"].append("Duplicate immutable_ids found in structure index")
        
        # Overall statistics
        validation_results["statistics"]["database_stats"] = db.get_statistics()
        
    except Exception as e:
        validation_results["valid"] = False
        validation_results["errors"].append(f"Validation failed: {str(e)}")
    
    return validation_results


def export_fingerprints_to_formats(
    data_dir: Union[str, Path], 
    output_dir: Union[str, Path] = None,
    formats: List[str] = None
) -> Dict[str, str]:
    """Export fingerprints to various formats for different use cases.
    
    Parameters
    ----------
    data_dir : str or Path
        Directory containing fingerprint files.
    output_dir : str or Path, optional
        Output directory. If None, uses data_dir.
    formats : List[str], optional
        List of formats to export. Options: ['txt', 'json', 'npy', 'hdf5']
        
    Returns
    -------
    Dict[str, str]
        Dictionary mapping format to output file path.
    """
    if formats is None:
        formats = ['txt', 'json']
    
    data_dir = Path(data_dir)
    output_dir = Path(output_dir) if output_dir else data_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load fingerprints
    db = FingerprintDatabase(data_dir)
    unique_fps = db.load_unique_fingerprints()
    
    if not unique_fps:
        raise ValueError("No fingerprints found to export")
    
    exported_files = {}
    
    # Convert to sorted list for consistent ordering
    fps_list = sorted(list(unique_fps))
    
    if 'txt' in formats:
        txt_file = output_dir / "unique_fingerprints.txt"
        with open(txt_file, 'w') as f:
            for fp in fps_list:
                f.write(f"{fp}\n")
        exported_files['txt'] = str(txt_file)
    
    if 'json' in formats:
        import json
        json_file = output_dir / "unique_fingerprints.json"
        with open(json_file, 'w') as f:
            json.dump(fps_list, f, indent=2)
        exported_files['json'] = str(json_file)
    
    if 'npy' in formats:
        import numpy as np
        npy_file = output_dir / "unique_fingerprints.npy"
        np.save(npy_file, fps_list)
        exported_files['npy'] = str(npy_file)
    
    if 'hdf5' in formats:
        try:
            import h5py
            hdf5_file = output_dir / "unique_fingerprints.h5"
            with h5py.File(hdf5_file, 'w') as f:
                # Store as variable-length strings
                dt = h5py.string_dtype(encoding='utf-8')
                f.create_dataset('fingerprints', data=fps_list, dtype=dt)
                f.attrs['count'] = len(fps_list)
                f.attrs['created'] = time.strftime('%Y-%m-%d %H:%M:%S')
            exported_files['hdf5'] = str(hdf5_file)
        except ImportError:
            logger.warning("h5py not available, skipping HDF5 export")
    
    return exported_files


def create_fingerprints_from_cifs(
    cif_folder: Union[str, Path],
    attach_to_structure: bool = True
) -> List[Structure]:
    """Create fingerprints for all CIF files in a folder.
    
    Parameters
    ----------
    cif_folder : str or Path
        Path to folder containing CIF files.
    attach_to_structure : bool, default=True
        Whether to attach fingerprint as attribute to Structure object.
        
    Returns
    -------
    List[Structure]
        List of Structure objects with fingerprints attached (if requested).
    """
    cif_folder = Path(cif_folder)
    if not cif_folder.exists():
        raise FileNotFoundError(f"CIF folder not found: {cif_folder}")
    
    # Find all CIF files
    cif_files = list(cif_folder.glob("*.cif"))
    if not cif_files:
        logger.warning(f"No CIF files found in {cif_folder}")
        return []
    
    logger.info(f"Processing {len(cif_files)} CIF files from {cif_folder}")
    
    structures = []
    failed_files = []
    
    for cif_file in cif_files:
        try:
            # Parse CIF file
            parser = CifParser(str(cif_file))
            structure = parser.get_structures()[0]  # Get first structure
            
            # Generate fingerprint
            try:
                fingerprint_tuple = get_augmented_fingerprint(structure)
                fingerprint_string = str(fingerprint_tuple)
                
                # Attach fingerprint to structure if requested
                if attach_to_structure:
                    structure.properties["augmented_fingerprint"] = fingerprint_string
                    structure.properties["fingerprint_tuple"] = fingerprint_tuple
                    structure.properties["source_file"] = str(cif_file.name)
                
                structures.append(structure)
                logger.debug(f"Processed {cif_file.name}: {fingerprint_string}")
                
            except Exception as e:
                logger.warning(f"Failed to generate fingerprint for {cif_file.name}: {e}")
                failed_files.append(cif_file.name)
                
        except Exception as e:
            logger.warning(f"Failed to parse CIF file {cif_file.name}: {e}")
            failed_files.append(cif_file.name)
    
    if failed_files:
        logger.warning(f"Failed to process {len(failed_files)} files: {failed_files}")
    
    logger.info(f"Successfully processed {len(structures)} structures")
    return structures


def check_novelty_against_lemat(
    structures: List[Structure],
    reference_data_dir: Union[str, Path] = "data/augmented_fingerprints"
) -> List[int]:
    """Check novelty of structures against LeMat-Bulk reference dataset.
    
    Parameters
    ----------
    structures : List[Structure]
        List of Structure objects to check for novelty.
        Must have 'augmented_fingerprint' in properties or will compute it.
    reference_data_dir : str or Path, default="data/augmented_fingerprints"
        Path to directory containing LeMat-Bulk reference data.
        
    Returns
    -------
    List[int]
        Binary list indicating novelty: 1 = novel, 0 = not novel.
        
    Examples
    --------
    >>> structures = [struct_A, struct_B, struct_C, struct_D]
    >>> novelty = check_novelty_against_lemat(structures)
    >>> print(novelty)  # [1, 1, 0, 0] if A,B are novel, C,D are not
    """
    if not structures:
        return []
    
    reference_data_dir = Path(reference_data_dir)
    
    # Load reference fingerprints
    db = FingerprintDatabase(reference_data_dir)
    reference_fingerprints = db.load_unique_fingerprints()
    
    if not reference_fingerprints:
        logger.warning("No reference fingerprints loaded, treating all as novel")
        return [1] * len(structures)
    
    logger.info(f"Checking novelty against {len(reference_fingerprints):,} reference fingerprints")
    
    novelty_results = []
    
    for i, structure in enumerate(structures):
        # Get fingerprint from structure properties or compute it
        fingerprint_string = structure.properties.get("augmented_fingerprint")
        
        if fingerprint_string is None:
            try:
                fingerprint_tuple = get_augmented_fingerprint(structure)
                fingerprint_string = str(fingerprint_tuple)
                # Cache it for future use
                structure.properties["augmented_fingerprint"] = fingerprint_string
                structure.properties["fingerprint_tuple"] = fingerprint_tuple
            except Exception as e:
                logger.warning(f"Failed to compute fingerprint for structure {i}: {e}")
                # Treat as novel if we can't compute fingerprint
                novelty_results.append(1)
                continue
        
        # Check novelty
        is_novel = fingerprint_string not in reference_fingerprints
        novelty_results.append(1 if is_novel else 0)
        
        logger.debug(f"Structure {i}: {'novel' if is_novel else 'not novel'}")
    
    novel_count = sum(novelty_results)
    logger.info(f"Novelty check complete: {novel_count}/{len(structures)} structures are novel")
    
    return novelty_results


def check_uniqueness_with_scores(
    structures: List[Structure]
) -> Tuple[List[float], List[Structure]]:
    """Check uniqueness of structures and return uniqueness scores.
    
    Parameters
    ----------
    structures : List[Structure]
        List of Structure objects to check for uniqueness.
        Must have 'augmented_fingerprint' in properties or will compute it.
        
    Returns
    -------
    Tuple[List[float], List[Structure]]
        - List of uniqueness scores (1/count_of_duplicate)
        - List of unique structures (one representative per unique fingerprint)
        
    Examples
    --------
    >>> structures = [A, B, C, C, D, D, D, D]  # C appears 2x, D appears 4x
    >>> scores, unique_structs = check_uniqueness_with_scores(structures)
    >>> print(scores)  # [1.0, 1.0, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25]
    >>> print(len(unique_structs))  # 4 (A, B, C, D)
    """
    if not structures:
        return [], []
    
    logger.info(f"Checking uniqueness for {len(structures)} structures")
    
    # Extract or compute fingerprints
    fingerprints = []
    for i, structure in enumerate(structures):
        fingerprint_string = structure.properties.get("augmented_fingerprint")
        
        if fingerprint_string is None:
            try:
                fingerprint_tuple = get_augmented_fingerprint(structure)
                fingerprint_string = str(fingerprint_tuple)
                # Cache it for future use
                structure.properties["augmented_fingerprint"] = fingerprint_string
                structure.properties["fingerprint_tuple"] = fingerprint_tuple
            except Exception as e:
                logger.warning(f"Failed to compute fingerprint for structure {i}: {e}")
                # Use structure index as fallback fingerprint
                fingerprint_string = f"FAILED_FINGERPRINT_{i}"
        
        fingerprints.append(fingerprint_string)
    
    # Count occurrences of each fingerprint
    fingerprint_counts = Counter(fingerprints)
    
    # Calculate uniqueness scores (1/count)
    uniqueness_scores = []
    for fp in fingerprints:
        count = fingerprint_counts[fp]
        score = 1.0 / count
        uniqueness_scores.append(score)
    
    # Get unique structures (one representative per unique fingerprint)
    unique_structures = []
    seen_fingerprints = set()
    
    for structure, fp in zip(structures, fingerprints):
        if fp not in seen_fingerprints:
            unique_structures.append(structure)
            seen_fingerprints.add(fp)
    
    # Log statistics
    total_unique_fps = len(fingerprint_counts)
    duplicates_info = {fp: count for fp, count in fingerprint_counts.items() if count > 1}
    
    logger.info("Uniqueness analysis complete:")
    logger.info(f"  Total structures: {len(structures)}")
    logger.info(f"  Unique fingerprints: {total_unique_fps}")
    logger.info(f"  Duplicate groups: {len(duplicates_info)}")
    
    if duplicates_info:
        logger.info("  Duplicate counts:")
        for fp, count in sorted(duplicates_info.items(), key=lambda x: x[1], reverse=True):
            # Show abbreviated fingerprint for readability
            fp_short = fp[:50] + "..." if len(fp) > 50 else fp
            logger.info(f"    {fp_short}: {count} copies")
    
    return uniqueness_scores, unique_structures


def analyze_structure_set(
    structures: List[Structure],
    reference_data_dir: Union[str, Path] = "data/augmented_fingerprints",
    attach_fingerprints: bool = True
) -> Dict[str, any]:
    """Comprehensive analysis of a set of structures for novelty and uniqueness.
    
    Parameters
    ----------
    structures : List[Structure]
        List of Structure objects to analyze.
    reference_data_dir : str or Path, default="data/augmented_fingerprints"
        Path to directory containing LeMat-Bulk reference data.
    attach_fingerprints : bool, default=True
        Whether to attach fingerprints to structures as properties.
        
    Returns
    -------
    Dict[str, any]
        Comprehensive analysis results including:
        - novelty_scores: List[int] - binary novelty indicators
        - uniqueness_scores: List[float] - uniqueness scores (1/count)
        - unique_structures: List[Structure] - unique representatives
        - statistics: Dict - summary statistics
    """
    if not structures:
        return {
            "novelty_scores": [],
            "uniqueness_scores": [],
            "unique_structures": [],
            "statistics": {"total_structures": 0}
        }
    
    logger.info(f"Starting comprehensive analysis of {len(structures)} structures")
    
    # Ensure all structures have fingerprints
    if attach_fingerprints:
        for i, structure in enumerate(structures):
            if "augmented_fingerprint" not in structure.properties:
                try:
                    fingerprint_tuple = get_augmented_fingerprint(structure)
                    fingerprint_string = str(fingerprint_tuple)
                    structure.properties["augmented_fingerprint"] = fingerprint_string
                    structure.properties["fingerprint_tuple"] = fingerprint_tuple
                except Exception as e:
                    logger.warning(f"Failed to compute fingerprint for structure {i}: {e}")
    
    # Check novelty
    novelty_scores = check_novelty_against_lemat(structures, reference_data_dir)
    
    # Check uniqueness
    uniqueness_scores, unique_structures = check_uniqueness_with_scores(structures)
    
    # Calculate statistics
    novel_count = sum(novelty_scores)
    unique_count = len(unique_structures)
    novel_and_unique_count = sum(
        1 for i, struct in enumerate(unique_structures)
        if novelty_scores[structures.index(struct)] == 1
    )
    
    statistics = {
        "total_structures": len(structures),
        "novel_structures": novel_count,
        "unique_structures": unique_count,
        "novel_and_unique_structures": novel_and_unique_count,
        "novelty_rate": novel_count / len(structures),
        "uniqueness_rate": unique_count / len(structures),
        "novel_uniqueness_rate": novel_and_unique_count / len(structures),
        "average_uniqueness_score": np.mean(uniqueness_scores),
        "duplicate_groups": len(structures) - unique_count
    }
    
    logger.info("Analysis complete!")
    logger.info(f"  Novel structures: {novel_count}/{len(structures)} ({statistics['novelty_rate']:.2%})")
    logger.info(f"  Unique structures: {unique_count}/{len(structures)} ({statistics['uniqueness_rate']:.2%})")
    logger.info(f"  Novel + Unique: {novel_and_unique_count}/{len(structures)} ({statistics['novel_uniqueness_rate']:.2%})")
    
    return {
        "novelty_scores": novelty_scores,
        "uniqueness_scores": uniqueness_scores,
        "unique_structures": unique_structures,
        "statistics": statistics
    }


def main():
    """Main function for fingerprint utilities."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Utilities for LeMat-Bulk fingerprints")
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Check command
    check_parser = subparsers.add_parser('check', help='Check fingerprint files status')
    check_parser.add_argument('data_dir', help='Directory containing fingerprint files')
    
    # Validate command
    validate_parser = subparsers.add_parser('validate', help='Validate fingerprint files integrity')
    validate_parser.add_argument('data_dir', help='Directory containing fingerprint files')
    
    # Stats command
    stats_parser = subparsers.add_parser('stats', help='Show fingerprint database statistics')
    stats_parser.add_argument('data_dir', help='Directory containing fingerprint files')
    
    # Export command
    export_parser = subparsers.add_parser('export', help='Export fingerprints to various formats')
    export_parser.add_argument('data_dir', help='Directory containing fingerprint files')
    export_parser.add_argument('--output-dir', help='Output directory for exported files')
    export_parser.add_argument('--formats', nargs='+', default=['txt', 'json'],
                              choices=['txt', 'json', 'npy', 'hdf5'],
                              help='Export formats')
    
    # Search command
    search_parser = subparsers.add_parser('search', help='Search structures by criteria')
    search_parser.add_argument('data_dir', help='Directory containing fingerprint files')
    search_group = search_parser.add_mutually_exclusive_group(required=True)
    search_group.add_argument('--formula', help='Search by chemical formula')
    search_group.add_argument('--elements', nargs='+', help='Search by element composition')
    search_group.add_argument('--spacegroup', type=int, help='Search by space group number')
    search_parser.add_argument('--output', help='Output file for search results (CSV)')
    
    # Analyze command
    analyze_parser = subparsers.add_parser('analyze', help='Analyze CIF files for novelty and uniqueness')
    analyze_parser.add_argument('cif_folder', help='Directory containing CIF files')
    analyze_parser.add_argument('--reference-dir', default='data/augmented_fingerprints',
                               help='Directory containing reference fingerprint data')
    analyze_parser.add_argument('--output', help='Output file for analysis results (JSON)')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # Handle different argument structures for different commands
    if args.command == 'analyze':
        # For analyze command, check if cif_folder exists
        cif_folder = Path(args.cif_folder)
        if not cif_folder.exists():
            print(f"âŒ Error: CIF folder not found: {cif_folder}")
            return
    else:
        # For other commands, check if data_dir exists
        data_dir = Path(args.data_dir)
        if not data_dir.exists():
            print(f"âŒ Error: Directory not found: {data_dir}")
            return
    
    if args.command == 'check':
        print(f"ðŸ” Checking fingerprint files in {data_dir}")
        file_status = check_fingerprint_files(data_dir)
        
        print("\nðŸ“ File Status:")
        for key, exists in file_status.items():
            if not key.endswith('_size_mb'):
                emoji = "âœ…" if exists else "âŒ"
                size_info = ""
                size_key = f"{key}_size_mb"
                if exists and size_key in file_status:
                    size_info = f" ({file_status[size_key]:.1f} MB)"
                print(f"   {emoji} {key}: {exists}{size_info}")
    
    elif args.command == 'validate':
        print(f"âœ… Validating fingerprint files in {data_dir}")
        results = validate_fingerprints_integrity(data_dir)
        
        if results['valid']:
            print("âœ… Validation passed!")
        else:
            print("âŒ Validation failed!")
        
        if results['errors']:
            print("\nðŸš¨ Errors:")
            for error in results['errors']:
                print(f"   â€¢ {error}")
        
        if results['warnings']:
            print("\nâš ï¸  Warnings:")
            for warning in results['warnings']:
                print(f"   â€¢ {warning}")
        
        print("\nðŸ“Š Statistics:")
        for key, value in results['statistics'].items():
            print(f"   â€¢ {key}: {value:,}" if isinstance(value, int) else f"   â€¢ {key}: {value}")
    
    elif args.command == 'stats':
        print(f"ðŸ“Š Loading fingerprint database from {data_dir}")
        db = FingerprintDatabase(data_dir)
        
        # Load all components
        db.load_unique_fingerprints()
        db.load_fingerprint_mapping()
        db.load_structure_index()
        
        stats = db.get_statistics()
        
        print("\nðŸ“ˆ Database Statistics:")
        for key, value in stats.items():
            if isinstance(value, dict):
                print(f"   â€¢ {key}:")
                for sub_key, sub_value in value.items():
                    print(f"     - {sub_key}: {sub_value}")
            else:
                print(f"   â€¢ {key}: {value:,}" if isinstance(value, int) else f"   â€¢ {key}: {value}")
    
    elif args.command == 'export':
        print(f"ðŸ“¤ Exporting fingerprints from {data_dir}")
        try:
            exported_files = export_fingerprints_to_formats(
                data_dir, 
                args.output_dir,
                args.formats
            )
            
            print("âœ… Export completed!")
            print("\nðŸ“„ Exported files:")
            for format_type, file_path in exported_files.items():
                file_size = Path(file_path).stat().st_size / (1024 * 1024)
                print(f"   â€¢ {format_type}: {file_path} ({file_size:.1f} MB)")
                
        except Exception as e:
            print(f"âŒ Export failed: {e}")
    
    elif args.command == 'search':
        print(f"ðŸ” Searching structures in {data_dir}")
        db = FingerprintDatabase(data_dir)
        
        if args.formula:
            results = db.search_by_formula(args.formula)
            print(f"Searching for formula: {args.formula}")
        elif args.elements:
            results = db.search_by_elements(args.elements)
            print(f"Searching for elements: {', '.join(args.elements)}")
        elif args.spacegroup:
            results = db.search_by_spacegroup(args.spacegroup)
            print(f"Searching for space group: {args.spacegroup}")
        
        print(f"\nðŸŽ¯ Found {len(results)} matching structures")
        
        if len(results) > 0:
            if len(results) <= 10:
                print("\nðŸ“‹ Results:")
                for _, row in results.iterrows():
                    print(f"   â€¢ {row['immutable_id']}: {row['formula']} (SG: {row.get('spacegroup_number', 'N/A')})")
            else:
                print("\nðŸ“‹ Showing first 10 results (use --output to save all):")
                for _, row in results.head(10).iterrows():
                    print(f"   â€¢ {row['immutable_id']}: {row['formula']} (SG: {row.get('spacegroup_number', 'N/A')})")
            
            if args.output:
                results.to_csv(args.output, index=False)
                print(f"\nðŸ’¾ Full results saved to: {args.output}")
    
    elif args.command == 'analyze':
        print(f"ðŸ”¬ Analyzing CIF files from {args.cif_folder}")
        
        try:
            # Create fingerprints from CIF files
            structures = create_fingerprints_from_cifs(cif_folder=args.cif_folder)
            
            if not structures:
                print("âŒ No structures were successfully processed")
                return
            
            print(f"âœ… Loaded {len(structures)} structures")
            
            # Perform comprehensive analysis
            results = analyze_structure_set(
                structures=structures,
                reference_data_dir=args.reference_dir
            )
            
            # Print summary
            stats = results["statistics"]
            print("\nðŸ“Š Analysis Results:")
            print(f"   Total structures: {stats['total_structures']}")
            print(f"   Novel structures: {stats['novel_structures']} ({stats['novelty_rate']:.1%})")
            print(f"   Unique structures: {stats['unique_structures']} ({stats['uniqueness_rate']:.1%})")
            print(f"   Novel + Unique: {stats['novel_and_unique_structures']} ({stats['novel_uniqueness_rate']:.1%})")
            print(f"   Average uniqueness score: {stats['average_uniqueness_score']:.3f}")
            print(f"   Duplicate groups: {stats['duplicate_groups']}")
            
            # Show novelty breakdown
            novelty_scores = results["novelty_scores"]
            print("\nðŸ” Novelty breakdown:")
            for i, (structure, is_novel) in enumerate(zip(structures, novelty_scores)):
                source_file = structure.properties.get("source_file", f"structure_{i}")
                status = "Novel" if is_novel else "Known"
                print(f"   â€¢ {source_file}: {status}")
            
            # Save detailed results if requested
            if args.output:
                import json
                
                # Prepare serializable results
                output_data = {
                    "analysis_timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
                    "cif_folder": str(args.cif_folder),
                    "reference_dir": str(args.reference_dir),
                    "statistics": stats,
                    "structure_details": []
                }
                
                # Add per-structure details
                for i, (structure, novelty, uniqueness) in enumerate(
                    zip(structures, results["novelty_scores"], results["uniqueness_scores"])
                ):
                    detail = {
                        "index": i,
                        "source_file": structure.properties.get("source_file", f"structure_{i}"),
                        "formula": structure.composition.reduced_formula,
                        "fingerprint": structure.properties.get("augmented_fingerprint"),
                        "is_novel": bool(novelty),
                        "uniqueness_score": float(uniqueness),
                        "num_sites": len(structure),
                        "density": float(structure.density)
                    }
                    output_data["structure_details"].append(detail)
                
                with open(args.output, 'w') as f:
                    json.dump(output_data, f, indent=2)
                
                print(f"\nðŸ’¾ Detailed results saved to: {args.output}")
                
        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()