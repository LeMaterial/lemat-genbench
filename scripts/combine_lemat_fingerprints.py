#!/usr/bin/env python3
"""Combine LeMat-Bulk fingerprint chunks into unified datasets.

This script combines all chunk files generated by preprocess_lemat_bulk_fingerprints.py
into unified formats for easier analysis and usage.
"""

import argparse
import glob
import json
import pickle
import time
from pathlib import Path
from typing import Dict

import pandas as pd
from tqdm import tqdm

from lemat_genbench.utils.logging import logger


class FingerprintCombiner:
    """Combine fingerprint chunks into unified datasets."""
    
    def __init__(self, data_dir: str, output_dir: str = None):
        """Initialize the combiner.
        
        Parameters
        ----------
        data_dir : str
            Directory containing fingerprint chunk files.
        output_dir : str, optional
            Output directory for combined files. If None, uses data_dir.
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir) if output_dir else self.data_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Statistics
        self.total_structures = 0
        self.successful_structures = 0
        self.unique_fingerprints = set()
        
    def combine_csvs(self, output_format: str = "parquet") -> str:
        """Combine all CSV chunk files into a single file.
        
        Parameters
        ----------
        output_format : str, default="parquet"
            Output format: "parquet" or "csv".
            
        Returns
        -------
        str
            Path to the combined file.
        """
        logger.info(f"Combining CSV files from {self.data_dir}")
        
        # Find all CSV chunk files
        csv_pattern = str(self.data_dir / "lemat_bulk_fingerprints_chunk_*.csv")
        csv_files = sorted(glob.glob(csv_pattern))
        
        if not csv_files:
            raise FileNotFoundError(f"No CSV chunk files found in {self.data_dir}")
        
        logger.info(f"Found {len(csv_files)} CSV chunk files")
        
        # Read and combine all CSV files
        combined_dfs = []
        
        for csv_file in tqdm(csv_files, desc="Reading CSV chunks"):
            try:
                df = pd.read_csv(csv_file)
                
                # Convert string lists back to proper format
                if 'elements' in df.columns:
                    df['elements'] = df['elements'].apply(
                        lambda x: x.split('|') if pd.notna(x) and x != '' else []
                    )
                if 'site_symmetries' in df.columns:
                    df['site_symmetries'] = df['site_symmetries'].apply(
                        lambda x: x.split('|') if pd.notna(x) and x != '' else []
                    )
                if 'multiplicity' in df.columns:
                    df['multiplicity'] = df['multiplicity'].apply(
                        lambda x: [int(i) for i in x.split('|')] if pd.notna(x) and x != '' else []
                    )
                
                combined_dfs.append(df)
                
                # Update statistics
                self.total_structures += len(df)
                self.successful_structures += df['success'].sum()
                
                # Collect unique fingerprints
                successful_fps = df[df['success']]['fingerprint_string'].dropna()
                self.unique_fingerprints.update(successful_fps)
                
            except Exception as e:
                logger.warning(f"Error reading {csv_file}: {e}")
                continue
        
        if not combined_dfs:
            raise ValueError("No valid CSV files could be read")
        
        # Combine all dataframes
        logger.info("Combining dataframes...")
        combined_df = pd.concat(combined_dfs, ignore_index=True)
        
        # Sort by immutable_id for consistency
        combined_df = combined_df.sort_values('immutable_id').reset_index(drop=True)
        
        # Save combined file
        if output_format.lower() == "parquet":
            output_file = self.output_dir / "lemat_bulk_fingerprints_combined.parquet"
            combined_df.to_parquet(output_file, index=False, engine='pyarrow')
        else:
            output_file = self.output_dir / "lemat_bulk_fingerprints_combined.csv"
            combined_df.to_csv(output_file, index=False)
        
        logger.info(f"Combined dataset saved to: {output_file}")
        logger.info(f"Total rows: {len(combined_df):,}")
        logger.info(f"Successful structures: {self.successful_structures:,}")
        logger.info(f"Success rate: {100 * self.successful_structures / self.total_structures:.2f}%")
        
        return str(output_file)
    
    def create_unique_fingerprints_file(self) -> str:
        """Create or update the unique fingerprints file.
        
        Returns
        -------
        str
            Path to the unique fingerprints file.
        """
        logger.info("Creating unique fingerprints file...")
        
        # Check if we already have unique fingerprints from CSV combination
        if not self.unique_fingerprints:
            logger.info("Loading fingerprints from chunk files...")
            
            # Load from pickle chunk files for more complete data
            pickle_pattern = str(self.data_dir / "lemat_bulk_fingerprints_chunk_*.pkl")
            pickle_files = sorted(glob.glob(pickle_pattern))
            
            if not pickle_files:
                raise FileNotFoundError(f"No pickle chunk files found in {self.data_dir}")
            
            for pickle_file in tqdm(pickle_files, desc="Reading pickle chunks"):
                try:
                    with open(pickle_file, 'rb') as f:
                        chunk_data = pickle.load(f)
                    
                    for item in chunk_data:
                        if item.get('success', False) and 'fingerprint_string' in item:
                            self.unique_fingerprints.add(item['fingerprint_string'])
                            
                except Exception as e:
                    logger.warning(f"Error reading {pickle_file}: {e}")
                    continue
        
        # Save unique fingerprints
        unique_fp_file = self.output_dir / "unique_fingerprints.pkl"
        with open(unique_fp_file, 'wb') as f:
            pickle.dump(self.unique_fingerprints, f)
        
        # Also save as text file for inspection
        unique_fp_txt = self.output_dir / "unique_fingerprints.txt"
        with open(unique_fp_txt, 'w') as f:
            for fp in sorted(self.unique_fingerprints):
                f.write(f"{fp}\n")
        
        logger.info(f"Unique fingerprints saved to: {unique_fp_file}")
        logger.info(f"Unique fingerprints count: {len(self.unique_fingerprints):,}")
        
        return str(unique_fp_file)
    
    def create_fingerprint_mapping(self) -> str:
        """Create a mapping from immutable_id to fingerprint_string.
        
        Returns
        -------
        str
            Path to the fingerprint mapping file.
        """
        logger.info("Creating fingerprint mapping...")
        
        fingerprint_mapping = {}
        
        # Load from pickle chunk files
        pickle_pattern = str(self.data_dir / "lemat_bulk_fingerprints_chunk_*.pkl")
        pickle_files = sorted(glob.glob(pickle_pattern))
        
        for pickle_file in tqdm(pickle_files, desc="Building fingerprint mapping"):
            try:
                with open(pickle_file, 'rb') as f:
                    chunk_data = pickle.load(f)
                
                for item in chunk_data:
                    if item.get('success', False) and 'fingerprint_string' in item:
                        immutable_id = item['immutable_id']
                        fingerprint = item['fingerprint_string']
                        fingerprint_mapping[immutable_id] = fingerprint
                        
            except Exception as e:
                logger.warning(f"Error reading {pickle_file}: {e}")
                continue
        
        # Save mapping
        mapping_file = self.output_dir / "fingerprint_mapping.pkl"
        with open(mapping_file, 'wb') as f:
            pickle.dump(fingerprint_mapping, f)
        
        # Also save as JSON for inspection
        mapping_json = self.output_dir / "fingerprint_mapping.json"
        with open(mapping_json, 'w') as f:
            json.dump(fingerprint_mapping, f, indent=2)
        
        logger.info(f"Fingerprint mapping saved to: {mapping_file}")
        logger.info(f"Mapping entries: {len(fingerprint_mapping):,}")
        
        return str(mapping_file)
    
    def create_structure_index(self) -> str:
        """Create an index of structures with key metadata for fast lookups.
        
        Returns
        -------
        str
            Path to the structure index file.
        """
        logger.info("Creating structure index...")
        
        structure_index = []
        
        # Load from pickle chunk files to get full structure data
        pickle_pattern = str(self.data_dir / "lemat_bulk_fingerprints_chunk_*.pkl")
        pickle_files = sorted(glob.glob(pickle_pattern))
        
        for pickle_file in tqdm(pickle_files, desc="Building structure index"):
            try:
                with open(pickle_file, 'rb') as f:
                    chunk_data = pickle.load(f)
                
                for item in chunk_data:
                    if item.get('success', False):
                        index_entry = {
                            'immutable_id': item['immutable_id'],
                            'fingerprint_string': item['fingerprint_string'],
                            'formula': item.get('formula', 'Unknown'),
                            'elements': item.get('elements', []),
                            'num_elements': item.get('num_elements', 0),
                            'num_sites': item.get('num_sites', 0),
                            'spacegroup_number': item.get('spacegroup_number'),
                            'density': item.get('density'),
                            'volume': item.get('volume'),
                            'chunk_num': item.get('chunk_num'),
                            'item_index': item.get('item_index')
                        }
                        structure_index.append(index_entry)
                        
            except Exception as e:
                logger.warning(f"Error reading {pickle_file}: {e}")
                continue
        
        # Convert to DataFrame and save
        index_df = pd.DataFrame(structure_index)
        
        # Save as parquet for efficient loading
        index_parquet = self.output_dir / "structure_index.parquet"
        index_df.to_parquet(index_parquet, index=False, engine='pyarrow')
        
        # Also save as CSV for inspection
        index_csv = self.output_dir / "structure_index.csv"
        index_df.to_csv(index_csv, index=False)
        
        logger.info(f"Structure index saved to: {index_parquet}")
        logger.info(f"Index entries: {len(index_df):,}")
        
        return str(index_parquet)
    
    def generate_summary_report(self) -> str:
        """Generate a comprehensive summary report.
        
        Returns
        -------
        str
            Path to the summary report.
        """
        logger.info("Generating summary report...")
        
        # Check for existing processing summary
        summary_file = self.data_dir / "processing_summary.json"
        processing_summary = {}
        if summary_file.exists():
            with open(summary_file, 'r') as f:
                processing_summary = json.load(f)
        
        # Create comprehensive report
        report = {
            "combination_timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "source_directory": str(self.data_dir),
            "output_directory": str(self.output_dir),
            "statistics": {
                "total_structures_processed": self.total_structures,
                "successful_structures": self.successful_structures,
                "failed_structures": self.total_structures - self.successful_structures,
                "success_rate_percent": 100 * self.successful_structures / self.total_structures if self.total_structures > 0 else 0,
                "unique_fingerprints_count": len(self.unique_fingerprints),
                "uniqueness_rate_percent": 100 * len(self.unique_fingerprints) / self.successful_structures if self.successful_structures > 0 else 0
            },
            "original_processing_summary": processing_summary,
            "files_created": {
                "combined_dataset": "lemat_bulk_fingerprints_combined.parquet",
                "unique_fingerprints": "unique_fingerprints.pkl",
                "fingerprint_mapping": "fingerprint_mapping.pkl", 
                "structure_index": "structure_index.parquet"
            }
        }
        
        # Save report
        report_file = self.output_dir / "combination_summary.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        logger.info(f"Summary report saved to: {report_file}")
        
        return str(report_file)
    
    def run_full_combination(self, output_format: str = "parquet") -> Dict[str, str]:
        """Run the complete combination process.
        
        Parameters
        ----------
        output_format : str, default="parquet"
            Format for the combined dataset.
            
        Returns
        -------
        Dict[str, str]
            Dictionary of created file paths.
        """
        logger.info("Starting full fingerprint combination process...")
        start_time = time.time()
        
        created_files = {}
        
        # 1. Combine CSV files
        try:
            combined_file = self.combine_csvs(output_format=output_format)
            created_files["combined_dataset"] = combined_file
        except Exception as e:
            logger.error(f"Failed to combine CSV files: {e}")
            raise
        
        # 2. Create unique fingerprints file
        try:
            unique_fp_file = self.create_unique_fingerprints_file()
            created_files["unique_fingerprints"] = unique_fp_file
        except Exception as e:
            logger.error(f"Failed to create unique fingerprints file: {e}")
            # Continue without this file
        
        # 3. Create fingerprint mapping
        try:
            mapping_file = self.create_fingerprint_mapping()
            created_files["fingerprint_mapping"] = mapping_file
        except Exception as e:
            logger.error(f"Failed to create fingerprint mapping: {e}")
            # Continue without this file
        
        # 4. Create structure index
        try:
            index_file = self.create_structure_index()
            created_files["structure_index"] = index_file
        except Exception as e:
            logger.error(f"Failed to create structure index: {e}")
            # Continue without this file
        
        # 5. Generate summary report
        try:
            report_file = self.generate_summary_report()
            created_files["summary_report"] = report_file
        except Exception as e:
            logger.error(f"Failed to generate summary report: {e}")
            # Continue without this file
        
        elapsed_time = time.time() - start_time
        
        # Print final summary
        print(f"\n{'='*80}")
        print("🎉 FINGERPRINT COMBINATION COMPLETED!")
        print(f"{'='*80}")
        print(f"⏱️  Total time: {elapsed_time:.2f} seconds")
        print(f"📊 Total structures: {self.total_structures:,}")
        print(f"✅ Successful structures: {self.successful_structures:,}")
        print(f"🔍 Unique fingerprints: {len(self.unique_fingerprints):,}")
        print(f"📈 Success rate: {100 * self.successful_structures / self.total_structures:.2f}%")
        print(f"🎯 Uniqueness rate: {100 * len(self.unique_fingerprints) / self.successful_structures:.2f}%")
        print(f"\n📁 Output directory: {self.output_dir}")
        print("📄 Created files:")
        for file_type, file_path in created_files.items():
            print(f"   • {file_type}: {Path(file_path).name}")
        
        return created_files


def main():
    """Main function for combining fingerprint chunks."""
    parser = argparse.ArgumentParser(
        description="Combine LeMat-Bulk fingerprint chunks into unified datasets"
    )
    parser.add_argument(
        "data_dir",
        help="Directory containing fingerprint chunk files"
    )
    parser.add_argument(
        "--output-dir",
        help="Output directory for combined files (default: same as data_dir)"
    )
    parser.add_argument(
        "--format",
        choices=["parquet", "csv"],
        default="parquet",
        help="Output format for combined dataset (default: parquet)"
    )
    parser.add_argument(
        "--csv-only",
        action="store_true",
        help="Only combine CSV files, skip other operations"
    )
    parser.add_argument(
        "--fingerprints-only",
        action="store_true", 
        help="Only create unique fingerprints file"
    )
    
    args = parser.parse_args()
    
    # Validate input directory
    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        raise FileNotFoundError(f"Data directory not found: {data_dir}")
    
    # Initialize combiner
    combiner = FingerprintCombiner(
        data_dir=str(data_dir),
        output_dir=args.output_dir
    )
    
    # Run combination based on arguments
    if args.csv_only:
        print("🔄 Combining CSV files only...")
        combined_file = combiner.combine_csvs(output_format=args.format)
        print(f"✅ Combined dataset saved to: {combined_file}")
        
    elif args.fingerprints_only:
        print("🔍 Creating unique fingerprints file only...")
        fp_file = combiner.create_unique_fingerprints_file()
        print(f"✅ Unique fingerprints saved to: {fp_file}")
        
    else:
        print("🚀 Running full combination process...")
        _ = combiner.run_full_combination(output_format=args.format)
        print("\n✅ All files created successfully!")


if __name__ == "__main__":
    main()